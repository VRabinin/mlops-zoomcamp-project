# CRM Sales Opportunities MLOps Platform

An end-to-end machine learning platform for predicting sales opportunities using CRM data. This project implements a complete MLOps pipeline with modern tools and practices for both local development and cloud production deployment.

## ğŸ¯ Objective

Build a production-ready machine learning system that:
- Predicts sales opportunity outcomes using CRM data
- Provides real-time predictions through a web interface
- Monitors model performance and data quality
- Supports continuous integration and deployment
- Scales from local development to cloud production

## ğŸ—ï¸ Solution Architecture

The platform follows a microservices-based architecture with the following key components:

### Core Technologies
- **User Interface**: Streamlit
- **Experiment Tracking**: MLFlow 2.7+ (PostgreSQL backend)
- **Workflow Orchestration**: Prefect 3.x (fully operational with CRM data pipeline)
- **Model Serving**: MLFlow
- **Storage**: MinIO S3-compatible storage (âœ… **OPERATIONAL** - 7.5MB+ CRM features stored)
- **Monitoring**: Evidently AI
- **CI/CD**: GitHub Actions
- **Infrastructure as Code**: Terraform
- **Container Orchestration**: HashiCorp Nomad

### Environment Support
- **Local Development**: Docker + MinIO
- **Production Deployment**: AWS Cloud

## ğŸ“ Project Structure

```
â”œâ”€â”€ .github/workflows/      # CI/CD pipelines
â”œâ”€â”€ .pre-commit-config.yaml # Pre-commit hooks configuration
â”œâ”€â”€ architecture/           # Solution architecture documentation
â”‚   â”œâ”€â”€ structurizr/       # Structurizr DSL Directory
â”‚   â”‚   â””â”€â”€ workspace.dsl  # C4 model in Structurizr DSL
â”‚   â”œâ”€â”€ README.md          # Architecture documentation
â”‚   â””â”€â”€ docker-compose.yml # Structurizr local setup
â”œâ”€â”€ config/                # Configuration files
â”‚   â””â”€â”€ development.yaml   # Development environment config
â”œâ”€â”€ data/                  # Data directories
â”‚   â”œâ”€â”€ raw/              # Raw data storage
â”‚   â”œâ”€â”€ processed/        # Processed data
â”‚   â””â”€â”€ features/         # Feature store
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ data/             # Data pipeline modules
â”‚   â”‚   â”œâ”€â”€ ingestion/    # Data ingestion (Kaggle CRM dataset) âœ… OPERATIONAL
â”‚   â”‚   â”‚   â”œâ”€â”€ crm_ingestion.py     # Monthly snapshot processing âœ…
â”‚   â”‚   â”‚   â””â”€â”€ crm_acquisition.py   # Enhanced data acquisition âœ…
â”‚   â”‚   â”œâ”€â”€ validation/   # Data quality validation âœ… OPERATIONAL (0.93 score)
â”‚   â”‚   â”œâ”€â”€ preprocessing/ # Feature engineering âœ… OPERATIONAL (23 features)
â”‚   â”‚   â””â”€â”€ schemas/      # Data schema definitions âœ…
â”‚   â”œâ”€â”€ pipelines/        # Prefect workflow definitions âœ… OPERATIONAL
â”‚   â”‚   â”œâ”€â”€ run_crm_ingestion.py     # Monthly CRM processing flow âœ…
â”‚   â”‚   â”œâ”€â”€ run_crm_acquisition.py   # Enhanced acquisition flow âœ…
â”‚   â”‚   â””â”€â”€ deploy_crm_pipelines.py  # S3-based deployment âœ…
â”‚   â”œâ”€â”€ utils/            # Utility modules âœ…
â”‚   â”‚   â””â”€â”€ storage.py    # Intelligent S3/local storage manager âœ…
â”‚   â””â”€â”€ config/           # Configuration management âœ…
â”‚   â”‚   â”œâ”€â”€ preprocessing/ # Feature engineering âœ… OPERATIONAL
â”‚   â”‚   â””â”€â”€ schemas/      # Data schema definitions âœ… OPERATIONAL
â”‚   â”œâ”€â”€ pipelines/        # Prefect workflow definitions âœ… OPERATIONAL
â”‚   â”‚   â”œâ”€â”€ run_crm_pipeline.py     # CRM data pipeline flow
â”‚   â”‚   â””â”€â”€ deploy_crm_pipeline.py  # Flow deployment scripts
â”‚   â””â”€â”€ config/           # Configuration management âœ… OPERATIONAL
â”œâ”€â”€ tests/                 # Test suites
â”œâ”€â”€ docker-compose.yml     # Local development services
â”œâ”€â”€ Makefile              # Development commands
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ .env.template         # Environment configuration template
```

## ğŸš€ Quick Start

Get the MLOps platform running locally with Docker in under 10 minutes.

### Prerequisites

- **Docker & Docker Compose**: v2+ with compose plugin
- **Python 3.11**: Required for optimal compatibility
- **Git**: For repository cloning
- **Kaggle Account**: For CRM dataset access

### Step 1: Clone and Setup

```bash
# Clone the repository
git clone https://github.com/VRabinin/mlops-zoomcamp-project.git
cd mlops-zoomcamp-project

# Complete environment setup (Python venv + dependencies + directories)
make dev-setup

# Activate virtual environment
source .venv/bin/activate
```

### Step 2: Configure Kaggle API

```bash
# Get API credentials from https://www.kaggle.com/account
# Download kaggle.json and place it:
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Create environment file from template
cp .env.template .env
# Edit .env with your Kaggle username and API key and adjust other variables if needed
```

### Step 3: Start Docker Services

```bash
# Start all MLOps infrastructure services
make application-start

# This starts:
# - PostgreSQL (MLflow backend)
# - MinIO (S3-compatible storage)
# - Redis (caching)
# - MLflow Server
# - Prefect Server

# Verify all services are running
make application-status
```

### Step 4: Run the Data Pipeline

```bash
#Deploy all pipeline parts to Prefect Server
make prefect-deploy

# Run the complete CRM data pipeline
make prefect-run

# This will:
# 1. Download CRM data from Kaggle (8,800+ records)
# 2. Validate data quality (0.93 score expected)
# 3. Engineer 23 ML features
# 4. Store results in MinIO S3 storage
```

### Step 5: Access the Platform

Open these services in your browser:

| Service | URL | Purpose |
|---------|-----|---------|
| **Prefect UI** | http://localhost:4200 | Workflow orchestration dashboard |
| **MLflow UI** | http://localhost:5000 | Experiment tracking & model registry |
| **MinIO Console** | http://localhost:9001 | S3 storage management |
| **Streamlit App** | http://localhost:8501 | ML predictions interface |

**Default Credentials:**
- MinIO: `minioadmin` / `minioadmin`
- PostgreSQL: `mlops_user` / `mlops_password`

### Step 6: Explore the Features

```bash
# View processed data in MinIO
make minio-ui
# Navigate to 'data-lake' bucket to see processed CRM features

# Check pipeline status
make prefect-status-all

# Launch the prediction web app
make streamlit-app
# Visit http://localhost:8501 for interactive ML predictions

# Start MLFlow UI for experiment tracking
make mlflow-ui
# Visit http://localhost:5000 for model registry and experiments
```

### That's It! ğŸ‰

You now have a complete MLOps platform running locally with:
- âœ… **Data Pipeline**: Automated CRM data processing
- âœ… **Experiment Tracking**: MLflow with PostgreSQL backend
- âœ… **Workflow Orchestration**: Prefect for pipeline automation
- âœ… **Object Storage**: MinIO S3-compatible storage
- âœ… **Web Interface**: Streamlit app for ML predictions
- âœ… **Monitoring**: Service health and pipeline status

### Step 7: View Architecture

```bash
# Start architecture viewer
make architecture-start
# Open http://localhost:8080 to view C4 diagrams (port configurable via STRUCTURIZR_PORT)

# Stop the architecture viewer
make architecture-stop
```

### Next Steps

```bash
# Explore the data
make jupyter-start  # Launch Jupyter notebooks

# Train ML models
make model-train    # Train win probability models

# Deploy flows for scheduling
make prefect-deploy-crm  # Deploy to Prefect server

# Monitor data quality
make monitoring-start    # Launch Evidently AI monitoring

# View comprehensive help
make help  # See all 30+ available commands
```

## ğŸ“Š Key Features

- **Real-time Predictions**: Web interface for immediate sales opportunity scoring
- **Automated Training**: Scheduled model retraining with new data
- **Model Monitoring**: Continuous monitoring of model performance and data drift
- **Experiment Tracking**: Complete ML experiment lifecycle management
- **Scalable Infrastructure**: Auto-scaling based on demand
- **Multi-environment Support**: Seamless local-to-cloud deployment

## ğŸ”§ Development Workflow

### Code Quality & Pre-commit Hooks

**âœ… Automated Code Quality**: This project uses pre-commit hooks to ensure consistent code quality:

```bash
# Pre-commit hooks are automatically installed with `make dev-setup`
# They run automatically before each commit to:
# - Format Python code with Black
# - Sort imports with isort
# - Run basic linting with flake8
# - Check YAML/JSON syntax
# - Remove trailing whitespace
# - Format Jupyter notebooks

# Manual pre-commit commands:
make precommit-run          # Run hooks on all files
make format                 # Format code manually
make lint                   # Run linting checks
make precommit-help         # Show all pre-commit options

# The hooks are configured to be developer-friendly:
# - Permissive linting rules to ease adoption
# - Automatic code formatting
# - Gradual quality improvement over time
```

### Daily Development Commands

```bash
# Start development environment with Prefect orchestration
make prefect-start      # Start Prefect server + agent (replaces dev-start)

# Alternative: Start individual services
make dev-start          # Start MLFlow + basic services
make prefect-server     # Start Prefect server only
make prefect-agent      # Start Prefect agent only

# Check project status
make status
make prefect-status-all # Enhanced status with Prefect info

# Run data pipeline (multiple options)
make data-pipeline              # Standalone execution
make data-pipeline-flow         # Prefect-orchestrated execution
make prefect-run-deployment     # Manual deployment trigger

# Deploy and manage Prefect workflows
make prefect-deploy-crm         # Deploy CRM flow to server
make prefect-deployments        # List all deployments
make prefect-flows             # Show recent flow runs

# Development and testing
make test               # Run test suite
make lint format        # Code quality checks
make precommit-run      # Run pre-commit hooks manually

# Monitoring and debugging
make prefect-ui         # Open Prefect dashboard
make mlflow-ui          # Open MLFlow dashboard
make prefect-help       # Show all Prefect commands

# Clean up
make clean
make prefect-stop       # Stop Prefect services
```

### Full Development Cycle

1. **Data Ingestion**: âœ… **OPERATIONAL** - Extract CRM data from Kaggle with validation using Prefect 3.x flows
2. **Feature Engineering**: âœ… **OPERATIONAL** - Transform raw data into 23 ML-ready features
3. **Model Training**: ğŸš§ **IN PROGRESS** - Train and evaluate ML models with Prefect orchestration
4. **Experiment Tracking**: âœ… **OPERATIONAL** - Log experiments and artifacts with MLFlow (PostgreSQL backend)
5. **Model Deployment**: ğŸ“‹ **PLANNED** - Deploy best models to serving infrastructure
6. **Monitoring**: ğŸ“‹ **PLANNED** - Track model performance with Evidently AI
7. **CI/CD**: âœ… **OPERATIONAL** - Automated testing and deployment with GitHub Actions

**Pipeline Achievements:**
- **Data Volume**: Processing 8,800+ CRM records successfully
- **Feature Engineering**: 23 engineered features from 8 original columns
- **Data Quality**: 0.93 validation score with comprehensive quality checks
- **Orchestration**: Prefect 3.x workflows with scheduling and monitoring
- **Infrastructure**: Docker Compose with PostgreSQL, Redis, and MinIO

### Available Make Commands

| Command | Description | Status |
|---------|-------------|--------|
| `make help` | Show all available commands | âœ… |
| `make dev-setup` | Complete development environment setup + pre-commit hooks | âœ… |
| `make prefect-start` | **Start Prefect server + agent (recommended)** | âœ… |
| `make dev-start` | Start MLFlow + basic services | âœ… |
| `make streamlit-app` | **Start Streamlit web application** | âœ… |
| `make streamlit-dev` | **Start Streamlit in development mode** | âœ… |
| `make data-pipeline` | Run complete data ingestion pipeline | âœ… |
| `make data-pipeline-flow` | **Run data pipeline as Prefect flow** | âœ… |
| `make prefect-deploy-crm` | **Deploy CRM ingestion flow to Prefect** | âœ… |
| `make prefect-run-deployment` | **Manually trigger CRM deployment** | âœ… |
| `make prefect-status-all` | **Show comprehensive Prefect status** | âœ… |
| `make prefect-deployments` | **List all Prefect deployments** | âœ… |
| `make prefect-flows` | **Show recent flow runs** | âœ… |
| `make prefect-ui` | **Open Prefect dashboard** | âœ… |
| `make prefect-help` | **Show all Prefect commands** | âœ… |
| `make test` | Run test suite | âœ… |
| `make lint` | Run code quality checks | âœ… |
| `make format` | Format code with Black and isort | âœ… |
| `make precommit-run` | Run pre-commit hooks on all files | âœ… |
| `make precommit-help` | Show all pre-commit commands | âœ… |
| `make architecture` | View architecture diagrams | âœ… |
| `make clean` | Clean temporary files | âœ… |

**âœ… New Prefect Commands**: 11 comprehensive workflow management commands added.

See `make help` for the complete list of 30+ commands.

## ğŸ“ˆ Monitoring & Observability

- Model performance metrics and alerts
- Data quality and drift detection
- Infrastructure and application monitoring
- Business impact tracking
- Automated retraining triggers

## ğŸ”’ Security & Compliance

- API authentication and authorization
- Data encryption at rest and in transit
- Audit logging and compliance reporting
- Network security and isolation
- Secret management

## ğŸ¤ Contributing

This project uses pre-commit hooks to maintain code quality. When you run `make dev-setup`, pre-commit hooks are automatically installed and will run before each commit to:

- **Format code**: Black for Python formatting, isort for import sorting
- **Quality checks**: Flake8 linting with developer-friendly rules
- **File validation**: YAML/JSON syntax, trailing whitespace removal
- **Notebook formatting**: Black formatting for Jupyter notebooks

The hooks are configured to be permissive initially to ease adoption. See `docs/PRE_COMMIT_GUIDE.md` for detailed information.

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Documentation

### Core Documentation
- [Architecture Overview](architecture/README.md) - Detailed system architecture
- [Next Steps Guide](NEXT_STEPS.md) - Development roadmap and priorities
- [Development Roadmap](ROADMAP.md) - Complete project timeline
- [Configuration Guide](.env.template) - Environment setup

### Development Guides
- [Pre-commit Guide](docs/PRE_COMMIT_GUIDE.md) - Code quality and pre-commit hooks
- [Prefect Workflow Integration](docs/PREFECT_WORKFLOW_INTEGRATION.md) - Workflow orchestration setup
- [Streamlit Docker Setup](docs/STREAMLIT_DOCKER_SETUP.md) - Containerized web app deployment
- [Streamlit Quickstart](docs/STREAMLIT_QUICKSTART.md) - Web application development guide

### Configuration References
- [Configurable Data Paths](docs/CONFIGURABLE_DATA_PATHS.md) - Data storage configuration
- [Storage Configuration](docs/STORAGE_CONFIGURATION.md) - S3/MinIO storage setup
- [Model Drift Monitoring](docs/MODEL_DRIFT_MONITORING.md) - Model performance monitoring

### Service URLs (Local Development)

| Service | URL | Purpose |
|---------|-----|---------|
| MLFlow UI | http://localhost:5000 | Experiment tracking and model registry |
| Prefect UI | http://localhost:4200 | Workflow orchestration dashboard |
| Architecture Viewer | http://localhost:8080* | C4 model diagrams |
| PostgreSQL | localhost:5432 | Database (mlops/mlops_user/mlops_password) |
| Redis | localhost:6379 | Caching and message broker |
| MinIO | http://localhost:9001 | S3-compatible object storage |

*Port configurable via `STRUCTURIZR_PORT` environment variable

## ğŸ› ï¸ Technology Stack

| Component | Technology | Version | Purpose | Status |
|-----------|------------|---------|---------|--------|
| **Runtime** | Python | 3.11+ | Core programming language | âœ… |
| **Frontend** | Streamlit | Latest | Web application interface | ğŸ“‹ |
| **ML Tracking** | MLFlow | 2.7+ | Experiment and model management | âœ… |
| **Orchestration** | Prefect | 3.x | Workflow automation | âœ… |
| **Monitoring** | Evidently AI | 0.4+ | Model and data monitoring | ğŸ“‹ |
| **CI/CD** | GitHub Actions | - | Automation pipeline | âœ… |
| **Infrastructure** | Terraform | Latest | Infrastructure as code | ğŸ“‹ |
| **Containers** | Docker + Nomad | Latest | Container orchestration | âœ… |
| **Database** | PostgreSQL | 15 | Structured data storage | âœ… |
| **Cache** | Redis | 7 | Caching and message broker | âœ… |
| **Storage** | MinIO/S3 | Latest | Object storage for artifacts | âœ… |
| **API Framework** | FastAPI | 0.104+ | Model serving API | ğŸ“‹ |

**Legend**: âœ… Operational | ğŸš§ In Progress | ğŸ“‹ Planned

## ğŸš¨ Troubleshooting

### Common Issues

**Python Version Issues:**
```bash
# Check your Python version
python --version

# If not 3.11, install it:
brew install python@3.11  # macOS
# or download from python.org
```

**Kaggle API Issues:**
```bash
# Verify Kaggle setup
kaggle datasets list
# Should show available datasets

# If authentication fails:
# 1. Check ~/.kaggle/kaggle.json exists
# 2. Verify file permissions: chmod 600 ~/.kaggle/kaggle.json
# 3. Check credentials are correct
```

**Docker Issues:**
```bash
# Check Docker is running
docker info

# Restart services if needed
docker compose down
docker compose up -d

# Check service logs
docker compose logs mlflow
docker compose logs postgres
```

**Port Conflicts:**
```bash
# Check what's using ports
lsof -i :5000  # MLFlow
lsof -i :4200  # Prefect
lsof -i :5432  # PostgreSQL
lsof -i :9000  # MinIO API
lsof -i :9001  # MinIO Console

# Kill processes if needed
sudo kill -9 <PID>
```

**MinIO S3 Storage Issues:**
```bash
# Check MinIO containers
docker ps | grep minio

# Check MinIO status
make minio-status

# List MinIO buckets
make minio-buckets

# Check MinIO logs
docker logs mlops-minio
docker logs mlops-minio-setup

# Access MinIO web UI
open http://localhost:9001
# Login: minioadmin / minioadmin

# Clear MinIO data if needed
make minio-clear-data
```

**Pre-commit Hook Issues:**
```bash
# Check if pre-commit hooks are installed
ls -la .git/hooks/pre-commit

# Reinstall pre-commit hooks if needed
make precommit-install

# Run hooks manually to test
make precommit-run

# Skip hooks temporarily (emergency only)
git commit --no-verify -m "message"

# Update hook versions
make precommit-update

# Show all pre-commit options
make precommit-help
```

### Getting Help

1. **Check service status**: `make status`
2. **View logs**: `docker compose logs <service-name>`
3. **Restart services**: `docker compose restart`
4. **Clean restart**: `make clean && make dev-setup`
5. **Check issues**: [GitHub Issues](https://github.com/VRabinin/mlops-zoomcamp-project/issues)

---

Built with â¤ï¸ for the MLOps community
